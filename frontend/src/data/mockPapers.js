export const mockPapers = [
  {
    id: "W2143456123",
    title: "Deep Residual Learning for Image Recognition",
    year: 2016,
    venue: "CVPR",
    authors: ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"],
    abstract:
      "Deeper neural networks are known to achieve better representational power, yet they are significantly harder to optimize due to vanishing gradients and degradation problems. In this work, we introduce a deep residual learning framework that reformulates layers as residual functions with reference to the layer inputs. This approach allows the construction of substantially deeper networks—up to hundreds or even thousands of layers—while maintaining efficient training and high accuracy.",
    rel: 0.94,
    oa: false,
    cited: 157234,
    refs: 38,
    link: "https://openalex.org/W2143456123"
  },
  {
    id: "W2755950974",
    title: "Adam: A Method for Stochastic Optimization",
    year: 2015,
    venue: "ICLR",
    authors: ["Diederik P. Kingma", "Jimmy Ba"],
    abstract:
      "Adam is an adaptive learning rate optimization algorithm designed for training deep neural networks. It combines momentum and RMSProp techniques, providing efficient and robust performance across a variety of tasks.",
    rel: 0.91,
    oa: true,
    cited: 120934,
    refs: 31,
    link: "https://openalex.org/W2755950974"
  },
  {
    id: "W2098459000",
    title: "Batch Normalization: Accelerating Deep Network Training",
    year: 2015,
    venue: "ICML",
    authors: ["Sergey Ioffe", "Christian Szegedy"],
    abstract:
      "Batch Normalization reduces internal covariate shift, enabling faster and more stable training of deep networks. It allows for higher learning rates and less sensitivity to initialization.",
    rel: 0.89,
    oa: true,
    cited: 84562,
    refs: 22,
    link: "https://openalex.org/W2098459000"
  },
  {
    id: "W2125635144",
    title: "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    year: 2014,
    venue: "JMLR",
    authors: ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever"],
    abstract:
      "Dropout is a regularization technique that prevents neural networks from overfitting by randomly omitting units during training. It leads to significant performance improvements on many benchmark datasets.",
    rel: 0.87,
    oa: true,
    cited: 93452,
    refs: 47,
    link: "https://openalex.org/W2125635144"
  },
  {
    id: "W2047829111",
    title: "Visualizing and Understanding Convolutional Networks",
    year: 2014,
    venue: "ECCV",
    authors: ["Matthew Zeiler", "Rob Fergus"],
    abstract:
      "We visualize convolutional neural networks to analyze how they encode image representations. The visualization reveals hierarchical features and provides insights into network architecture design.",
    rel: 0.85,
    oa: true,
    cited: 56347,
    refs: 27,
    link: "https://openalex.org/W2047829111"
  },
  {
    id: "W2184907722",
    title: "Understanding the Difficulty of Training Deep Feedforward Neural Networks",
    year: 2010,
    venue: "AISTATS",
    authors: ["Xavier Glorot", "Yoshua Bengio"],
    abstract:
      "We investigate the challenges of training deep networks and propose an initialization strategy that maintains variance across layers. This approach improves gradient flow and accelerates convergence.",
    rel: 0.83,
    oa: true,
    cited: 46528,
    refs: 30,
    link: "https://openalex.org/W2184907722"
  },
  {
    id: "W2298846675",
    title: "Understanding Deep Learning Requires Rethinking Generalization",
    year: 2017,
    venue: "ICLR",
    authors: ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht"],
    abstract:
      "We challenge classical generalization theories by demonstrating that deep networks can perfectly fit random labels. This calls for a re-evaluation of overfitting and generalization in deep learning.",
    rel: 0.82,
    oa: true,
    cited: 38210,
    refs: 33,
    link: "https://openalex.org/W2298846675"
  },
  {
    id: "W2313342781",
    title: "Efficient BackProp",
    year: 1998,
    venue: "Neural Networks: Tricks of the Trade",
    authors: ["Yann LeCun", "Léon Bottou", "Genevieve Orr", "Klaus-Robert Müller"],
    abstract:
      "We provide practical recommendations for training deep neural networks effectively, including learning rate selection, normalization, and gradient scaling. These techniques form the foundation of modern deep learning optimization.",
    rel: 0.80,
    oa: true,
    cited: 29562,
    refs: 41,
    link: "https://openalex.org/W2313342781"
  },
  {
    id: "W2391100922",
    title: "ReLU: Rectified Linear Units Improve Restricted Boltzmann Machines",
    year: 2010,
    venue: "ICML",
    authors: ["Vinod Nair", "Geoffrey Hinton"],
    abstract:
      "We introduce Rectified Linear Units (ReLU) as activation functions for deep networks. ReLUs accelerate convergence and improve representational capacity compared to traditional sigmoid neurons.",
    rel: 0.79,
    oa: true,
    cited: 22841,
    refs: 21,
    link: "https://openalex.org/W2391100922"
  },
  {
    id: "W2434511102",
    title: "A Beginner’s Guide to Deep Learning: Concepts and Applications",
    year: 2021,
    venue: "Applied Artificial Intelligence",
    authors: ["Laura Smith", "Hiro Tanaka"],
    abstract:
      "This tutorial-oriented paper introduces deep learning principles for beginners, including network design, optimization, and evaluation strategies, with practical examples from vision and NLP.",
    rel: 0.77,
    oa: true,
    cited: 4312,
    refs: 29,
    link: "https://openalex.org/W2434511102"
  }
];
